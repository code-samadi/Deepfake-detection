{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:07:59.262032Z",
     "start_time": "2024-12-16T15:07:59.116050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Custom dataset for FFHQ\n",
    "class FFHQDataset(Dataset):\n",
    "    def __init__(self, hr_dir, lr_dir, transform=None):\n",
    "        self.hr_dir = hr_dir  # High-Resolution images\n",
    "        self.lr_dir = lr_dir  # Low-Resolution images\n",
    "        self.hr_images = os.listdir(hr_dir)\n",
    "        self.lr_images = os.listdir(lr_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hr_path = os.path.join(self.hr_dir, self.hr_images[idx])\n",
    "        lr_path = os.path.join(self.lr_dir, self.lr_images[idx])\n",
    "        \n",
    "        hr_image = Image.open(hr_path).convert('RGB')\n",
    "        lr_image = Image.open(lr_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            hr_image = self.transform(hr_image)\n",
    "            lr_image = self.transform(lr_image)\n",
    "        \n",
    "        return lr_image, hr_image"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset, DataLoader\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transforms\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:07:53.454088Z",
     "start_time": "2024-12-16T15:07:53.443846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Path to the dataset directory\n",
    "dataset_path = \"/kaggle/input/flickrfaceshq-dataset-ffhq\"  # Adjust based on where the dataset is located\n",
    "\n",
    "# List all files and subdirectories inside the dataset\n",
    "print(dataset_path)\n",
    "print(len(dataset_path))"
   ],
   "id": "8d30e124723871a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/flickrfaceshq-dataset-ffhq\n",
      "40\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "# from byol_pytorch import BYOL\n",
    "from torchvision import models\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image"
   ],
   "id": "2d28fdeffbff1a84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.file_names = os.listdir(root_dir)\n",
    "        self.file_names.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.root_dir, self.file_names[idx])\n",
    "        image = Image.open(file_path)\n",
    "        if self.transform:\n",
    "            image = self.tran"
   ],
   "id": "bd9dbee75cc8bac2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = CustomDataset('/kaggle/input/flickrfaceshq-dataset-ffhq', transform=transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "print(len(dataloader))"
   ],
   "id": "400643c022c989de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "# Path to the dataset directory\n",
    "dataset_path = \"/kaggle/input/flickrfaceshq-dataset-ffhq\"  # Update this with the correct path\n",
    "\n",
    "# List of valid image extensions (you can add more if needed)\n",
    "valid_image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "\n",
    "# List to store paths of image files\n",
    "image_files = []\n",
    "\n",
    "# Walk through all directories and subdirectories\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        # Check if the file is an image based on the extension\n",
    "        if file.lower().endswith(valid_image_extensions):\n",
    "            image_files.append(os.path.join(root, file))\n",
    "\n",
    "# Print total number of image files found\n",
    "print(f\"Total number of image files: {len(image_files)}\")\n"
   ],
   "id": "6ce4ba0105e52cc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Path to the high-resolution images (HR images)\n",
    "hr_dir = \"/kaggle/input/flickrfaceshq-dataset-ffhq\"  # Adjust this path based on the actual structure\n",
    "\n",
    "# Path to save low-resolution images (LR images)\n",
    "lr_dir = \"/kaggle/working/lr-images\"  # Adjust as needed (ensure it's writable)\n",
    "\n",
    "# Create LR directory if it doesn't exista\n",
    "if not os.path.exists(lr_dir):\n",
    "    os.makedirs(lr_dir)\n",
    "\n",
    "resize_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# Downscale HR images to generate LR images (e.g., 256x256)\n",
    "count = 0  # Counter for processed images\n",
    "for img_name in os.listdir(hr_dir):\n",
    "    img_path = os.path.join(hr_dir, img_name)\n",
    "    \n",
    "    if os.path.isfile(img_path):  # Check if it's a valid file\n",
    "        try:\n",
    "            # Open image and convert to RGB\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Apply transformation\n",
    "            img_tensor = resize_transform(img).to(device)  # Move image to GPU\n",
    "            \n",
    "            # Convert back to PIL image\n",
    "            img_resized = transforms.ToPILImage()(img_tensor.cpu())  # Move back to CPU\n",
    "            \n",
    "            # Save resized image\n",
    "            img_resized.save(os.path.join(lr_dir, img_name))\n",
    "            \n",
    "            count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_name}: {e}\")\n",
    "\n",
    "print(f\"Done! Converted {count} images to low resolution and saved to {lr_dir}.\")\n",
    "Using device: cuda"
   ],
   "id": "95a2628c739ed539"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lr_dir = \"/kaggle/input/lr-images-root\"",
   "id": "6f733d0df099f80b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ],
   "id": "c77785d640269355"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class FFHQDataset(Dataset):\n",
    "    def __init__(self, hr_dir, lr_dir, transform=None):\n",
    "        self.hr_dir = hr_dir\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_images = sorted(os.listdir(hr_dir))\n",
    "        self.lr_images = sorted(os.listdir(lr_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.hr_images), len(self.lr_images))  # Ensure lengths match\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hr_path = os.path.join(self.hr_dir, self.hr_images[idx])\n",
    "        lr_path = os.path.join(self.lr_dir, self.lr_images[idx])\n",
    "        \n",
    "        hr_image = Image.open(hr_path).convert('RGB')\n",
    "        lr_image = Image.open(lr_path).convert('RGB')\n",
    "\n",
    "        # Resize to expected dimensions if necessary\n",
    "        hr_image = hr_image.resize((512, 512), Image.Resampling.LANCZOS)\n",
    "        lr_image = lr_image.resize((256, 256), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        if self.transform:\n",
    "            hr_image = self.transform(hr_image)\n",
    "            lr_image = self.transform(lr_image)\n",
    "        \n",
    "        return lr_image, hr_image"
   ],
   "id": "d7f1967ab457e5ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Number of HR images: {len(dataset.hr_images)}\")\n",
    "print(f\"Number of LR images: {len(dataset.lr_images)}\")"
   ],
   "id": "a3dd18b83e842e65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for idx in range(len(dataset)):\n",
    "    lr_image, hr_image = dataset[idx]\n",
    "    print(f\"LR Image {idx}: {lr_image.size}, HR Image {idx}: {hr_image.size}\")\n",
    "    break"
   ],
   "id": "fe86967bdbbbc1dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple ESRGAN-like model for super-resolution\n",
    "class ESRGANGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ESRGANGenerator, self).__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3 * 4, kernel_size=3, padding=1),  # For 2x upscaling\n",
    "            nn.PixelShuffle(2)  # 2x upscaling\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)"
   ],
   "id": "71c40ae4adb3c6f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize between -1 and 1\n",
    "    ])\n",
    "dataset = FFHQDataset(hr_dir=hr_dir, lr_dir=lr_dir, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ],
   "id": "11386be591c751c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(model, train_loader, device, num_epochs=1, lr=0.0001):\n",
    "    criterion = nn.MSELoss()  # Loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Optimizer\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for lr_images, hr_images in train_loader:\n",
    "            lr_images = lr_images.to(device)\n",
    "            hr_images = hr_images.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(lr_images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, hr_images)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "# Step 4: Train the model\n",
    "device = torch.device(\"cud"
   ],
   "id": "3496911c74d3af57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 4: Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ESRGANGenerator().to(device)\n",
    "train_model(model, train_loader, device, num_epochs=1)"
   ],
   "id": "9bbbb9dce0a6349c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
